{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# DeepSeek-V3.1 Model Getting Started Guide on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive introduction to using DeepSeek-V3.1 on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for this advanced reasoning model. \n",
    "\n",
    "## Model Overview\n",
    "\n",
    "### DeepSeek-V3.1\n",
    "\n",
    "**Parameters:** 685 billion (Mixture-of-Experts hybrid model)\n",
    "\n",
    "**Use Cases:** Complex reasoning tasks, agentic use cases, thinking and non-thinking modes\n",
    "\n",
    "**Key Features:**\n",
    "- **Thinking Mode**: Carefully works through problems step-by-step with enhanced reasoning\n",
    "- **Non-Thinking Mode**: Provides quick responses to straightforward questions\n",
    "- **Hybrid Architecture**: Mixture-of-Experts (MoE) design for optimal performance\n",
    "- **Enhanced Tool Calling**: Superior performance in agent-based tasks\n",
    "\n",
    "## Core Capabilities\n",
    "\n",
    "DeepSeek-V3.1 offers the following characteristics:\n",
    "\n",
    "**Input/Output:** Text-in, text-out \n",
    "\n",
    "**Context Window:** 128,000 tokens  \n",
    "\n",
    "**Model Type:** Advanced reasoning model with thinking capabilities\n",
    "\n",
    "**Languages:** English and Chinese\n",
    "\n",
    "**Tool Calling:** ‚úÖ Supported (Enhanced capabilities)\n",
    "\n",
    "**Bedrock Guardrails** ‚úÖ Supported\n",
    "\n",
    "**Converse API** ‚úÖ Supported\n",
    "\n",
    "**OpenAI Chat Completions API** ‚úÖ Supported\n",
    "\n",
    "**Streaming:** ‚úÖ Supported\n",
    "\n",
    "**Model Evaluation:** ‚úÖ Supported\n",
    "\n",
    "**Agents:** ‚úÖ Supported\n",
    "\n",
    "**Prompt Management:** ‚úÖ Supported\n",
    "\n",
    "**Flows:** ‚úÖ Supported\n",
    "\n",
    "**Batch Inference:** ‚úÖ Supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## What You'll Learn in this getting started guide\n",
    "\n",
    "- Options to use Amazon Bedrock for DeepSeek-V3.1 inference, including:    \n",
    "    - Using the OpenAI SDK with Amazon Bedrock\n",
    "    - Using Amazon Bedrock's InvokeModel API\n",
    "    - Using Amazon Bedrock's Converse API\n",
    "- Understanding request parameters and response structures\n",
    "- Leveraging thinking vs non-thinking modes for different use cases\n",
    "- Implementing enhanced tool calling capabilities\n",
    "- Exploring reasoning capabilities with thinking mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Access on Amazon Bedrock\n",
    "\n",
    "Ensure you have the correct IAM permission in order to access DeepSeek's models on Amazon Bedrock. \n",
    "\n",
    "## IAM Permissions\n",
    "\n",
    "To use Bedrock models, your AWS credentials need the following permissions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": \"arn:aws:bedrock:*::foundation-model/deepseek.v3-v1:0\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** The wildcard (`*`) in the region field covers all supported regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
    "\n",
    "### Required Imports:\n",
    "- `os` ‚Üí For environment variables\n",
    "- `boto3` ‚Üí For native Bedrock API interactions  \n",
    "- `json` ‚Üí For JSON serialization/deserialization\n",
    "- `datetime` ‚Üí For timestamp tracking and performance measurements\n",
    "- `openai` ‚Üí For OpenAI SDK compatibility with Bedrock\n",
    "- `IPython.display` ‚Üí For enhanced output formatting and streaming demonstrations\n",
    "\n",
    "### Environment Variables:\n",
    "We set two environment variables to redirect the OpenAI SDK:\n",
    "- `AWS_BEARER_TOKEN_BEDROCK` ‚Üí Your Bedrock API key  \n",
    "- `OPENAI_BASE_URL` ‚Üí Bedrock's OpenAI-compatible endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 openai ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Model ID\n",
    "\n",
    "- **deepseek.v3-v1:0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - DeepSeek-V3.1\n",
    "MODEL_ID = \"deepseek.v3-v1:0\"  \n",
    "\n",
    "print(f\"‚úÖ Using model: {MODEL_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables to point to Bedrock\n",
    "# Note: Change the region in the URL to match your preferred region\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"bedrock-api-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"bedrock-api-key\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
    "\n",
    "print(\"‚úÖ Environment configured for Bedrock!\")\n",
    "print(\"üìç Using us-west-2 region - change the URL above to use a different region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Inference with Amazon Bedrock\n",
    "\n",
    "### Option 1: OpenAI SDK\n",
    "\n",
    "#### Import and Initialize OpenAI Client\n",
    "\n",
    "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
    "\n",
    "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both clients\n",
    "# Note: Change region_name to match your preferred region\n",
    "client = OpenAI()  # For chat completions API\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized (pointing to Bedrock)\")\n",
    "print(f\"‚úÖ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")\n",
    "print(\"üìç Change region_name above to use a different supported region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Make API Calls \n",
    "\n",
    "The API call structure is identical to OpenAI:\n",
    "- Same `messages` format with `role` and `content`\n",
    "- Same `model` parameter (but uses Bedrock model IDs)  \n",
    "- Same `stream` parameter for real-time responses\n",
    "- **New**: `thinking_mode` parameter to control thinking vs non-thinking behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Non-thinking mode (quick response)\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1000,\n",
    "    # Non-thinking mode for quick responses\n",
    "\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(\"ü§ñ Non-thinking mode response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Thinking mode (step-by-step reasoning)\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"If a train leaves station A at 60 mph and another leaves station B at 40 mph, and they are 200 miles apart, when will they meet?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=2000,\n",
    "    # reasoning_effort values: low, medium, high\n",
    "    reasoning_effort='high'  # Thinking mode for complex reasoning\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(\"üß† Thinking mode response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Process Streaming Response\n",
    "\n",
    "Handle the response exactly like you would with OpenAI's SDK. Each `item` in the response is a chunk of the model's output. DeepSeek-V3.1 supports streaming in both thinking and non-thinking modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with thinking mode\n",
    "streaming_response = client.chat.completions.create(\n",
    "    model=MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"Explain how photosynthesis works in simple terms.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1500,\n",
    "    reasoning_effort='high',  # Enable thinking mode\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "print(\"üß† Streaming thinking mode response:\")\n",
    "for chunk in streaming_response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### What's Happening Behind the Scenes?\n",
    "\n",
    "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
    "\n",
    "#### Request Translation\n",
    "- **OpenAI SDK Request** ‚Üí **Bedrock InvokeModel** \n",
    "- The request body structure remains the same\n",
    "- But there are some key differences in how parameters are handled:\n",
    "\n",
    "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
    "|-----------|------------|-------------------|\n",
    "| **Model ID** | In request body | Part of the URL path |\n",
    "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>‚Ä¢ `InvokeModel` (non-streaming)<br/>‚Ä¢ `InvokeModelWithResponseStream` (streaming) |\n",
    "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |\n",
    "| **Thinking Mode** | `thinking_mode=True/False` | DeepSeek-V3.1 specific parameter |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Enhanced Function Calling with OpenAI SDK\n",
    "\n",
    "DeepSeek-V3.1 features enhanced tool calling capabilities for superior performance in agent-based tasks. Let's demonstrate this with a weather lookup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    \"\"\"\n",
    "    Get current weather for a given location.\n",
    "    This is a mock function that returns sample weather data for demonstration.\n",
    "    \n",
    "    Args:\n",
    "        location (str): City name or \"City, Country\" format\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather information with temperature, condition, and humidity\n",
    "    \"\"\"\n",
    "    # Enhanced mock weather data - in a real application, you'd call a weather API\n",
    "    weather_data = {\n",
    "        # Exact matches\n",
    "        \"Paris, France\": {\"temperature\": \"22¬∞C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"Tokyo, Japan\": {\"temperature\": \"25¬∞C\", \"condition\": \"Light rain\", \"humidity\": \"80%\"},\n",
    "        \"New York, USA\": {\"temperature\": \"18¬∞C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Sydney, Australia\": {\"temperature\": \"28¬∞C\", \"condition\": \"Clear skies\", \"humidity\": \"55%\"},\n",
    "        \"London, UK\": {\"temperature\": \"15¬∞C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
    "        \n",
    "        # City-only matches for more flexible parsing\n",
    "        \"Paris\": {\"temperature\": \"22¬∞C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"Tokyo\": {\"temperature\": \"25¬∞C\", \"condition\": \"Light rain\", \"humidity\": \"80%\"},\n",
    "        \"New York\": {\"temperature\": \"18¬∞C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Sydney\": {\"temperature\": \"28¬∞C\", \"condition\": \"Clear skies\", \"humidity\": \"55%\"},\n",
    "        \"London\": {\"temperature\": \"15¬∞C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(location, {\n",
    "        \"temperature\": \"23¬∞C\", \n",
    "        \"condition\": \"Pleasant\", \n",
    "        \"humidity\": \"60%\"\n",
    "    })\n",
    "\n",
    "# Define the function schema for OpenAI SDK\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country, e.g. 'Paris, France'\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"‚úÖ Weather function and tools configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_functions(client, model, messages, tools, max_iterations=3, reasoning_effort='high'):\n",
    "    \"\"\"\n",
    "    Chat with function calling support using OpenAI SDK format.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        model: Model ID to use\n",
    "        messages: List of conversation messages\n",
    "        tools: List of available tools/functions\n",
    "        max_iterations: Maximum number of function call iterations\n",
    "        reasoning_effort: Whether to use thinking mode for enhanced reasoning\n",
    "        \n",
    "    Returns:\n",
    "        Final assistant message\n",
    "    \"\"\"\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"üîÑ Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Make request with tools\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            reasoning_effort=reasoning_effort  # Use thinking mode for enhanced tool calling\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # Check if the model wants to call functions\n",
    "        if assistant_message.tool_calls:\n",
    "            print(f\"üîß Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
    "            \n",
    "            # Process each function call\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"üîß Calling function: {function_name}\")\n",
    "                print(f\"üîß Arguments: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"get_weather\":\n",
    "                    function_result = get_weather(function_args[\"location\"])\n",
    "                    print(f\"üîß Function result: {function_result}\")\n",
    "                else:\n",
    "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                function_message = {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": json.dumps(function_result)\n",
    "                }\n",
    "                messages.append(function_message)\n",
    "                \n",
    "        else:\n",
    "            # No more function calls, return final response\n",
    "            print(\"‚úÖ No function calls requested, conversation complete\")\n",
    "            return assistant_message\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Maximum iterations reached\")\n",
    "    return assistant_message\n",
    "\n",
    "print(\"‚úÖ Enhanced function calling handler ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test enhanced function calling with thinking mode\n",
    "weather_questions = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"Can you tell me the temperature in Tokyo?\",\n",
    "    \"How's the weather in Sydney, Australia?\",\n",
    "    \"What are the conditions like in New York?\"\n",
    "]\n",
    "\n",
    "print(\"üå§Ô∏è Testing Enhanced Function Calling with DeepSeek-V3.1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(weather_questions, 1):\n",
    "    print(f\"\\nüìù Test {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. When users ask about weather, temperature, or weather conditions for any location, you MUST call the get_weather function to get current information. Always use the function when weather information is requested.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # Test function calling directly with tool_choice=\"required\"\n",
    "        print(\"üîß Testing function call...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"required\",  # Force function calling\n",
    "            reasoning_effort='high'\n",
    "        )\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            print(f\"‚úÖ Function called successfully: {tool_call.function.name}\")\n",
    "            print(f\"üîß Arguments: {tool_call.function.arguments}\")\n",
    "            \n",
    "            # Execute the function and show result\n",
    "            import json\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            function_result = get_weather(args.get(\"location\", \"Unknown\"))\n",
    "            print(f\"üîß Function result: {function_result}\")\n",
    "            \n",
    "            # Generate a dummy response based on the function result\n",
    "            location = args.get(\"location\", \"Unknown\")\n",
    "            temp = function_result.get(\"temperature\", \"Unknown\")\n",
    "            condition = function_result.get(\"condition\", \"Unknown\")\n",
    "            humidity = function_result.get(\"humidity\", \"Unknown\")\n",
    "            \n",
    "            dummy_response = f\"Based on current weather data for {location}: Temperature is {temp} with {condition.lower()} conditions and {humidity} humidity. Great for planning your day!\"\n",
    "            \n",
    "            print(\"ü§ñ Generated weather report:\")\n",
    "            print(dummy_response)\n",
    "            print(\"‚úÖ Function calling demonstration complete!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå No function call made\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### What Just Happened with Enhanced Function Calling?\n",
    "\n",
    "The enhanced function calling demonstration shows DeepSeek-V3.1's improved capabilities:\n",
    "\n",
    "1. **Enhanced Tool Recognition**: DeepSeek-V3.1 has superior tool calling performance for agent-based tasks\n",
    "2. **Thinking Mode Integration**: When `thinking_mode=True`, the model can reason through complex tool selection and usage\n",
    "3. **Improved Function Execution**: Better understanding of when and how to use available tools\n",
    "4. **Multi-step Reasoning**: The model can plan and execute complex multi-tool workflows\n",
    "5. **Context Awareness**: Enhanced understanding of conversation context for better tool usage decisions\n",
    "\n",
    "**Key Advantages of DeepSeek-V3.1's Enhanced Tool Calling:**\n",
    "- More accurate tool selection based on user intent\n",
    "- Better handling of complex multi-step agent workflows  \n",
    "- Improved reasoning about tool parameters and results\n",
    "- Enhanced error handling and recovery in tool usage scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Option 2: Amazon Bedrock's InvokeModel API\n",
    "\n",
    "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
    "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
    "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
    "- Privacy: Amazon Bedrock does not store your input or output data‚Äîrequests are used only for inference.\n",
    "- **Thinking Mode Control**: Direct control over DeepSeek-V3.1's thinking vs non-thinking behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Setup client\n",
    "\n",
    "First, we setup the Amazon Bedrock client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure region for Bedrock client\n",
    "region = \"us-west-2\"\n",
    "\n",
    "if region is None:\n",
    "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "else:\n",
    "    target_region = \"us-west-2\"\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
    "print(f\"üìç Using region: {target_region} - change the region variable above to use a different supported region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Inference with InvokeModel API\n",
    "\n",
    "Then we use the InvokeModel API to perform model inference with DeepSeek-V3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    \"\"\"\n",
    "    Invokes Amazon bedrock model to run an inference\n",
    "    using the input provided in the request body.\n",
    "    \n",
    "    Args:\n",
    "        body (dict): The invokation body to send to bedrock\n",
    "        model_id (str): the model to query\n",
    "        accept (str): input accept type\n",
    "        content_type (str): content type\n",
    "    Returns:\n",
    "        Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with thinking mode enabled\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "]\n",
    "\n",
    "body = {\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 1000,\n",
    "    \"reasoning_effort\": \"high\"  # Enable thinking mode for DeepSeek-V3.1\n",
    "}\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, MODEL_ID, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(\"üß† Thinking mode response:\")\n",
    "print(response_body['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Streaming with InvokeModel API\n",
    "\n",
    "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. DeepSeek-V3.1 supports streaming in both thinking and non-thinking modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with thinking mode\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "]\n",
    "\n",
    "body = {\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 1000,\n",
    "    \"reasoning_effort\": \"high\"  # Enable thinking mode for streaming\n",
    "}\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    body=json.dumps(body), modelId=MODEL_ID, accept=accept, contentType=contentType\n",
    ")\n",
    "chunk_count = 0\n",
    "time_to_first_token = None\n",
    "\n",
    "# Process the response stream\n",
    "stream = response.get(\"body\")\n",
    "if stream:\n",
    "    print(\"üß† Streaming thinking mode response:\")\n",
    "    for event in stream:\n",
    "        chunk = event.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # Print the response chunk\n",
    "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "            content_block_delta = chunk_json.get(\"choices\")[0][\"delta\"].get(\"content\")\n",
    "            if content_block_delta:\n",
    "                if time_to_first_token is None:\n",
    "                    time_to_first_token = datetime.now() - start_time\n",
    "                    print(f\"Time to first token: {time_to_first_token}\")\n",
    "\n",
    "                chunk_count += 1\n",
    "                print(content_block_delta, end=\"\")\n",
    "    print(f\"\\nTotal chunks: {chunk_count}\")\n",
    "else:\n",
    "    print(\"No response stream received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Option 3: Amazon Bedrock's Converse API\n",
    "\n",
    "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes. \n",
    "\n",
    "Key Benefits:\n",
    "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
    "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
    "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
    "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management\n",
    "- **Thinking Mode Support**: Direct control over DeepSeek-V3.1's thinking capabilities\n",
    "\n",
    "Additionally, the Converse API automatically separates the reasoning trace from the final response, giving developers the flexibility to show or hide the model's thinking process from end users based on their application needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converse API with thinking mode\n",
    "response = bedrock_client.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How far from earth is the moon?\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0,\n",
    "        \"maxTokens\": 1000\n",
    "    },\n",
    "    additionalModelRequestFields={\n",
    "        \"reasoning_effort\": \"high\"  # Enable thinking mode for DeepSeek-V3.1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Message dict\n",
    "print(f\"üìù Message dict:\")\n",
    "print(response['output']['message']['content'])\n",
    "\n",
    "# Reasoning trace (if available)\n",
    "if 'reasoningContent' in response['output']['message']['content'][0]:\n",
    "    print(f\"üìù Reasoning trace:\")\n",
    "    print(response['output']['message']['content'][0]['reasoningContent']['reasoningText']['text'])\n",
    "\n",
    "# Final response\n",
    "print(f\"üìù Final response:\")\n",
    "print(response['output']['message']['content'][1]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Streaming with Converse API\n",
    "\n",
    "The Converse API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. The output below will contain reasoning trace and final response. Based on your application you might want to hide the reasoning trace from the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedrock_model_converse_stream(client, system_prompt, user_prompt, max_tokens=1000, temperature=0, reasoning_effort='high'):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },                        \n",
    "        ],\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens\n",
    "        },\n",
    "        additionalModelRequestFields={\n",
    "            \"reasoning_effort\": reasoning_effort  # Enable thinking mode for DeepSeek-V3.1\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            if chunk['delta'].get('reasoningContent', None):\n",
    "                print(chunk['delta']['reasoningContent']['text'], end=\"\")\n",
    "            if chunk['delta'].get('text', None):\n",
    "                print(chunk['delta']['text'], end=\"\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input prompts\n",
    "system_prompt = \"You are a concise, highly logical assistant.\"\n",
    "user_prompt = \"How far from earth is the moon?\"\n",
    "\n",
    "# Invoke model through Converse API with streaming and thinking mode\n",
    "print(\"üß† Streaming with thinking mode:\")\n",
    "bedrock_model_converse_stream(bedrock_client, system_prompt, user_prompt, reasoning_effort='high')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully explored **three powerful ways** to interact with DeepSeek-V3.1 on Amazon Bedrock, including comprehensive tool use capabilities and thinking mode!\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "**1. OpenAI SDK Integration**\n",
    "- Set up the familiar OpenAI SDK to work seamlessly with AWS Bedrock\n",
    "- Leveraged existing OpenAI patterns while running on AWS infrastructure\n",
    "- Demonstrated streaming responses with real-time token generation\n",
    "- **Implemented enhanced function calling** using familiar OpenAI SDK patterns\n",
    "- **Explored thinking vs non-thinking modes** for different use cases\n",
    "\n",
    "**2. Direct InvokeModel API Access**\n",
    "- Implemented low-level Bedrock InvokeModel API calls for maximum control\n",
    "- Built custom functions for both streaming and non-streaming inference\n",
    "- Measured performance metrics like time-to-first-token for streaming responses\n",
    "- Gained fine-grained control over model parameters and request formatting\n",
    "- **Built custom tool use handling** with manual request/response processing\n",
    "- **Direct control over thinking mode** through request parameters\n",
    "\n",
    "**3. Bedrock Converse API**\n",
    "- Explored the unified Converse API that works across all Bedrock models\n",
    "- Demonstrated consistent message-based interactions regardless of underlying model\n",
    "- Leveraged built-in support for system prompts and inference configuration\n",
    "- **Integrated tool calling** using Bedrock's native toolSpec format\n",
    "- **Thinking mode integration** through additionalModelRequestFields\n",
    "\n",
    "**4. Enhanced Capabilities**\n",
    "- Implemented the **same weather function across all three APIs** for direct comparison\n",
    "- Learned the different schema formats and response handling approaches\n",
    "- Understood when to choose each API based on your specific needs\n",
    "- **Explored DeepSeek-V3.1's unique thinking capabilities**\n",
    "- **Demonstrated enhanced tool calling performance**\n",
    "\n",
    "### Key Benefits Achieved\n",
    "\n",
    "‚úÖ **Flexibility**: Three different API approaches for different use cases  \n",
    "‚úÖ **Performance**: Streaming support for improved user experience  \n",
    "‚úÖ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
    "‚úÖ **Control**: Direct API access when you need fine-grained customization  \n",
    "‚úÖ **Consistency**: Universal interface that works across all Bedrock models  \n",
    "‚úÖ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
    "‚úÖ **Tool Integration**: Enhanced function calling capabilities across all three approaches\n",
    "‚úÖ **Practical Comparison**: Side-by-side examples using the same function\n",
    "‚úÖ **Thinking Mode**: Step-by-step reasoning capabilities for complex problems\n",
    "‚úÖ **Hybrid Architecture**: Mixture-of-Experts design for optimal performance\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now equipped with comprehensive knowledge to choose the right API approach for your specific use case. Whether you need:\n",
    "- The **simplicity** of the OpenAI SDK\n",
    "- The **control** of InvokeModel \n",
    "- The **consistency** of Converse API\n",
    "- **Enhanced tool use capabilities** for external integrations\n",
    "- **Thinking mode** for complex reasoning tasks\n",
    "- **Non-thinking mode** for quick responses\n",
    "\n",
    "You have all the tools and examples to build powerful AI applications with DeepSeek-V3.1's advanced capabilities on Amazon Bedrock!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
