{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0c804-1b42-4b4d-958b-1dfb33c483f4",
   "metadata": {},
   "source": [
    "# Deploy DeepSeek R1 Llama on AWS Inferentia using SageMaker Large Model Inference Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a5386-536a-4977-9ffc-988519db65d3",
   "metadata": {},
   "source": [
    "In this example you will deploy the model  `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203b9e8-e332-4629-881a-476c98a8eb44",
   "metadata": {},
   "source": [
    "Update the sagemaker SDK to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6796a3-cb84-44ad-863d-48fab445d543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969d8df-2897-4598-8a66-ed73f103059a",
   "metadata": {},
   "source": [
    "Baseline setup- Set Session id, role and region for the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7e0ab-084f-47fe-8638-bf3028d56a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107bede-071b-433c-972d-ddbf9b00e47b",
   "metadata": {},
   "source": [
    "Download the model snapshot from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50753c0a-5354-4b85-9a69-f6f5de6192ce",
   "metadata": {},
   "source": [
    "Create a unique model name based prefix to prevent name collisions on future runs from the HuggingFace Model ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891c65d3-9c4b-440c-880a-9c6a6701925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Llama-8B-0128202751\n"
     ]
    }
   ],
   "source": [
    "#pull out the model name\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "ts = time.time()\n",
    "base_model_name = f\"{hf_model_id.rsplit('/', 1)[-1].replace('.','-')}-{datetime.datetime.fromtimestamp(ts).strftime('%m%d%H%M%S')}\"\n",
    "\n",
    "print(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9c20a-57e1-49fa-87b3-7e70a3f8b794",
   "metadata": {},
   "source": [
    "## Large Model Inference (LMI) Containers\n",
    "\n",
    "In this example you will deploy your model using [SageMaker's Large Model Inference (LMI) Containers](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html).\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution.\n",
    "\n",
    "For this example we will use the LMI container with vLLM backend. \n",
    "\n",
    "The model for this example can be deployed using the vLLM backend, which corresponds to the `djl-lmi` container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660ac53-d3db-4464-ac58-9440b5cc70f8",
   "metadata": {},
   "source": [
    "| Backend | SageMakerDLC | Example URI |\n",
    "| --- | --- | --- |\n",
    "|vLLM|djl-lmi|763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.30.0-neuronx-sdk2.20.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d459e9-0471-45e4-ad91-788f650f73ef",
   "metadata": {},
   "source": [
    "Supplying the `LMI_VERSION` along with the desirect `LMI_FRAMEWORK` below will fetch the corresponding ECR image for deploying your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638a764e-572b-4dc6-b062-7a2bce3521c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.30.0-neuronx-sdk2.20.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.30.0-neuronx-sdk2.20.1\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3f713-e1cc-459a-ade8-eaf77aabce1d",
   "metadata": {},
   "source": [
    "Next you will need to specify configuration of the LMI container to allow the model artifact to be downloaded, and provide optimized parameters to allow the model to run on the chosen instance size/type.\n",
    "\n",
    "There are 2 methods to supply configuration to the LMI container:\n",
    "1. Create a `serving.properties` file and include it inside the compressed model artifact. This has the benefit of ensuring that no configuration information needs to be shared, as long as you have the model artifact. However it creates rigidity as it is tightly coupled and creates complexity when deploying on different instance types.\n",
    "2. Provide a set of Environment Variables to the SageMaker Model object. This provides flexibility by storing the LMI configuration information inside the SageMaker Model configuration step.\n",
    "\n",
    "In this example, you will leverage Environment Variables to configure the LMI container.\n",
    "\n",
    "For deploying HuggingFace models, the `HF_MODEL_ID` parameter is dual purpose and can be either the HuggingFace Model ID, or an S3 location of the model artifacts. If you specify the Model ID, the artifacts will be downloaded when the endpoint is created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69ee651-add3-45ac-a2f7-99a6c0ac55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "##Add your Huggingface model token below\n",
    "vllm_config = {\n",
    "    \"HF_MODEL_ID\": hf_model_id,\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"HF_TOKEN\": \"\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_OUTPUT_FORMATTER\": \"json\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_MODEL_LOADING_TIMEOUT\": \"1600\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78440331-00b0-4516-9538-cdd155d41e21",
   "metadata": {},
   "source": [
    "In the following steps you will leverage the SageMaker Python SDK to build your model configuration and deploy it to SageMaker endpoint. There are alternative methods to do this as well, such as the Boto3 SDK, but the SM Python SDK reduces the amount of code necessary perform the same activities.\n",
    "\n",
    "The first step in model deployment is to [create a SageMaker Model object](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html). This consists of a unique name, a container image, and the environment configuration from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e963df6-9da4-4279-9c3a-2db6373ff9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = f\"{base_model_name}\"\n",
    "model_name = base_model_name\n",
    "\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri = inference_image_uri,\n",
    "    env = vllm_config,\n",
    "    role = role,\n",
    "    name = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15e14-dd62-4b7a-8e3d-2f5bc6bcff14",
   "metadata": {},
   "source": [
    "Now that you have a model object ready, you will use use the SageMaker Python SDK to create a SageMaker Managed Endpoint. The SDK eliminates some of the intermediate steps, such as creating an Endpoint Configuration.\n",
    "\n",
    "## Note: creating a new endpoint can take between 8-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d83fe6-05c6-4e99-a859-a3371f6b7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "endpoint_name = f\"{model_name}-endpoint-v1\"\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    container_startup_health_check_timeout = 1600,\n",
    "    endpoint_name = endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf1e3c-7338-4ff7-8a8b-8bccaf8711b2",
   "metadata": {},
   "source": [
    "With your endpoint successfully deployed, you will want to test it to ensure that it is fully functional.\n",
    "\n",
    "To do so, you will take a piece of sample text and summarize it using your deployed model. This sample text was pulled from the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dee8d92-efab-4120-9481-977a19bb46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_food = \"\"\"\n",
    "How to make cake?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful chef assistant who is an expert in screating recipes.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Create a recipe here.\n",
    "\n",
    "{recipe_food}\n",
    "\n",
    "\n",
    "Provide the summary directly, without any introduction or preamble. Do not start the response with \"Here is a...\".<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c4f02-5ed9-464c-8e42-bf2fc06e0656",
   "metadata": {},
   "source": [
    "Using the sample article and prompt template, invoke the model to view the structure of the response and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6256f9f-6885-4091-9c61-4eacb536488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Llama-8B-0128202751-endpoint-v1\n",
      "CPU times: user 17.6 ms, sys: 7.72 ms, total: 25.3 ms\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, the user wants me to create a recipe for cake. Let me think about the basic ingredients and steps needed. I should make sure to include everything clearly so the user can follow easily. I'll start with the ingredients, listing each one with their quantities. Then, I'll outline the steps in a logical order: preheating the oven, mixing dry ingredients, adding wet ingredients, pouring into the pan, baking, and cooling. I need to keep the language simple and straightforward without any unnecessary words. That should cover everything the user needs to make a perfect cake.\\n</think>\\n\\n**Cake Recipe**\\n\\n**Ingredients:**\\n- 1 cup (2 sticks) butter, softened\\n- 3/4 cup granulated sugar\\n- 1/4 cup packed brown sugar\\n- 4 large eggs\\n- 1 cup whole milk\\n- 1 teaspoon vanilla extract\\n- 2 1/4 cups all-purpose flour\\n- 1 teaspoon baking powder\\n- 1/2 teaspoon baking soda\\n- 1/2 teaspoon salt\\n- 2 cups powdered sugar (for dusting)\\n\\n**Instructions:**\\n1. Preheat the oven to 350°F (175°C).\\n2. In a large bowl, beat the butter and sugars until\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "print(endpoint_name)\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "response = llm.predict(\n",
    "    {\n",
    "        \"inputs\": prompt_template,\n",
    "        \"parameters\": {\n",
    "            \"do_sample\":True,\n",
    "            \"max_new_tokens\":256,\n",
    "            \"top_p\":0.9,\n",
    "            \"temperature\":0.6,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response['generated_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
