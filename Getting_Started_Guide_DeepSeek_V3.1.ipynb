{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DeepSeek-V3.1 Model Getting Started Guide on Amazon Bedrock\n",
        "\n",
        "This notebook provides a comprehensive introduction to using DeepSeek-V3.1 on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for this advanced reasoning model. \n",
        "\n",
        "## Model Overview\n",
        "\n",
        "### DeepSeek-V3.1\n",
        "\n",
        "**Parameters:** 685 billion (Mixture-of-Experts hybrid model)\n",
        "\n",
        "**Use Cases:** Complex reasoning tasks, agentic use cases, thinking and non-thinking modes\n",
        "\n",
        "**Key Features:**\n",
        "- **Thinking Mode**: Carefully works through problems step-by-step with enhanced reasoning\n",
        "- **Non-Thinking Mode**: Provides quick responses to straightforward questions\n",
        "- **Hybrid Architecture**: Mixture-of-Experts (MoE) design for optimal performance\n",
        "- **Enhanced Tool Calling**: Superior performance in agent-based tasks\n",
        "\n",
        "## Core Capabilities\n",
        "\n",
        "DeepSeek-V3.1 offers the following characteristics:\n",
        "\n",
        "**Input/Output:** Text-in, text-out \n",
        "\n",
        "**Context Window:** 128,000 tokens  \n",
        "\n",
        "**Model Type:** Advanced reasoning model with thinking capabilities\n",
        "\n",
        "**Languages:** English and Chinese\n",
        "\n",
        "**Supported Regions:** \n",
        "- **US:** us-east-1 (N. Virginia), us-west-2 (Oregon)\n",
        "- **EU:** eu-west-1 (Ireland), eu-west-2 (London), eu-north-1 (Stockholm), eu-south-1 (Milan)\n",
        "- **AP:** ap-northeast-1 (Tokyo), ap-south-1 (Mumbai)\n",
        "- **SA:** sa-east-1 (São Paulo)\n",
        "\n",
        "**Tool Calling:** ✅ Supported (Enhanced capabilities)\n",
        "\n",
        "**Bedrock Guardrails** ✅ Supported\n",
        "\n",
        "**Converse API** ✅ Supported\n",
        "\n",
        "**OpenAI Chat Completions API** ✅ Supported\n",
        "\n",
        "**Streaming:** ✅ Supported\n",
        "\n",
        "**Model Evaluation:** ✅ Supported\n",
        "\n",
        "**Agents:** ✅ Supported\n",
        "\n",
        "**Prompt Management:** ✅ Supported\n",
        "\n",
        "**Flows:** ✅ Supported\n",
        "\n",
        "**Batch Inference:** ✅ Supported"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What You'll Learn in this getting started guide\n",
        "\n",
        "- Options to use Amazon Bedrock for DeepSeek-V3.1 inference, including:    \n",
        "    - Using the OpenAI SDK with Amazon Bedrock\n",
        "    - Using Amazon Bedrock's InvokeModel API\n",
        "    - Using Amazon Bedrock's Converse API\n",
        "- Understanding request parameters and response structures\n",
        "- Leveraging thinking vs non-thinking modes for different use cases\n",
        "- Implementing enhanced tool calling capabilities\n",
        "- Exploring reasoning capabilities with thinking mode\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Access on Amazon Bedrock\n",
        "\n",
        "Ensure you have the correct IAM permission in order to access DeepSeek's models on Amazon Bedrock. \n",
        "\n",
        "## IAM Permissions\n",
        "\n",
        "To use Bedrock models, your AWS credentials need the following permissions:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "```json\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"bedrock:InvokeModel\",\n",
        "        \"bedrock:InvokeModelWithResponseStream\"\n",
        "      ],\n",
        "      \"Resource\": \"arn:aws:bedrock:*::foundation-model/deepseek.v3-v1:0\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Note:** The wildcard (`*`) in the region field covers all supported regions:\n",
        "- **US:** us-east-1, us-west-2\n",
        "- **EU:** eu-west-1, eu-west-2, eu-north-1, eu-south-1  \n",
        "- **AP:** ap-northeast-1, ap-south-1\n",
        "- **SA:** sa-east-1\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Configuration\n",
        "\n",
        "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
        "\n",
        "### Required Imports:\n",
        "- `os` → For environment variables\n",
        "- `boto3` → For native Bedrock API interactions  \n",
        "- `json` → For JSON serialization/deserialization\n",
        "- `datetime` → For timestamp tracking and performance measurements\n",
        "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
        "- `strands` → For Amazon Strands agent framework\n",
        "- `IPython.display` → For enhanced output formatting and streaming demonstrations\n",
        "\n",
        "### Environment Variables:\n",
        "We set two environment variables to redirect the OpenAI SDK:\n",
        "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
        "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in ./.venv/lib/python3.13/site-packages (1.40.34)\n",
            "Collecting openai\n",
            "  Downloading openai-1.108.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: ipython in ./.venv/lib/python3.13/site-packages (9.5.0)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.34 in ./.venv/lib/python3.13/site-packages (from boto3) (1.40.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.venv/lib/python3.13/site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in ./.venv/lib/python3.13/site-packages (from boto3) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.venv/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.34->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./.venv/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.34->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.34->boto3) (1.17.0)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "Collecting sniffio (from openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting typing-extensions<5,>=4.11 (from openai)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: decorator in ./.venv/lib/python3.13/site-packages (from ipython) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.13/site-packages (from ipython) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.13/site-packages (from ipython) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.13/site-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.13/site-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.13/site-packages (from ipython) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.13/site-packages (from ipython) (2.19.2)\n",
            "Requirement already satisfied: stack_data in ./.venv/lib/python3.13/site-packages (from ipython) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.13/site-packages (from ipython) (5.14.3)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.13/site-packages (from jedi>=0.16->ipython) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in ./.venv/lib/python3.13/site-packages (from stack_data->ipython) (0.2.3)\n",
            "Downloading openai-1.108.0-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.1/948.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading jiter-0.11.0-cp313-cp313-macosx_11_0_arm64.whl (314 kB)\n",
            "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Installing collected packages: typing-extensions, tqdm, sniffio, jiter, idna, h11, distro, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [openai]15/16\u001b[0m [openai]c]-types]\n",
            "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.11.0 openai-1.108.0 pydantic-2.11.9 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 openai ipython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output, display, display_markdown, Markdown\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Model ID\n",
        "\n",
        "- **deepseek.v3-v1:0**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using model: deepseek.v3-v1:0\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration - DeepSeek-V3.1\n",
        "MODEL_ID = \"deepseek.v3-v1:0\"  \n",
        "\n",
        "print(f\"✅ Using model: {MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment configured for Bedrock!\n",
            "📍 Using us-west-2 region - change the URL above to use a different region\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables to point to Bedrock\n",
        "# Note: Change the region in the URL to match your preferred region\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"ABSKQmVkcm9ja0FQSUtleS15eXJqLWF0LTQwNTY0NTIyMjcyODpHVkRMbWpZQXNRQXlVd2cwc0RmSXpCeWdXY3J5OGVONFpQaDVncXVFVVh1b2k0UXdhbHZiVFYwK2Vycz0=\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"ABSKQmVkcm9ja0FQSUtleS15eXJqLWF0LTQwNTY0NTIyMjcyODpHVkRMbWpZQXNRQXlVd2cwc0RmSXpCeWdXY3J5OGVONFpQaDVncXVFVVh1b2k0UXdhbHZiVFYwK2Vycz0=\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
        "\n",
        "print(\"✅ Environment configured for Bedrock!\")\n",
        "print(\"📍 Using us-west-2 region - change the URL above to use a different region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Inference with Amazon Bedrock\n",
        "\n",
        "### Option 1: OpenAI SDK\n",
        "\n",
        "#### Import and Initialize OpenAI Client\n",
        "\n",
        "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
        "\n",
        "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI client initialized (pointing to Bedrock)\n",
            "✅ Bedrock client initialized in region: us-west-2\n",
            "📍 Change region_name above to use a different supported region\n"
          ]
        }
      ],
      "source": [
        "# Initialize both clients\n",
        "# Note: Change region_name to match your preferred region\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "client = OpenAI()  # For chat completions API\n",
        "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
        "\n",
        "print(\"✅ OpenAI client initialized (pointing to Bedrock)\")\n",
        "print(f\"✅ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")\n",
        "print(\"📍 Change region_name above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Make API Calls \n",
        "\n",
        "The API call structure is identical to OpenAI:\n",
        "- Same `messages` format with `role` and `content`\n",
        "- Same `model` parameter (but uses Bedrock model IDs)  \n",
        "- Same `stream` parameter for real-time responses\n",
        "- **New**: `thinking_mode` parameter to control thinking vs non-thinking behavior\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Non-thinking mode response:\n",
            "The largest city in the southern hemisphere is São Paulo, Brazil. It has a metropolitan population of over 22 million people.\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Non-thinking mode (quick response)\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1000,\n",
        "    # Non-thinking mode for quick responses\n",
        "\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"🤖 Non-thinking mode response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Thinking mode response:\n",
            "<reasoning>First, the problem is about two trains moving towards each other from different stations. Train A leaves station A at 60 mph, and train B leaves station B at 40 mph. The distance between the stations is 200 miles. I need to find out when they will meet.\n",
            "\n",
            "Since they are moving towards each other, their speeds will add up when considering the relative speed. The relative speed is the sum of their speeds because they're approaching each other. So, relative speed = 60 mph + 40 mph = 100 mph.\n",
            "\n",
            "The distance between them is 200 miles. To find the time it takes for them to meet, I use the formula: time = distance / relative speed.\n",
            "\n",
            "So, time = 200 miles / 100 mph = 2 hours.\n",
            "\n",
            "Therefore, they will meet after 2 hours.\n",
            "\n",
            "I should double-check. Let's think about the distances each train travels in that time.\n",
            "\n",
            "In 2 hours, train A at 60 mph will travel 60 * 2 = 120 miles.\n",
            "\n",
            "Train B at 40 mph will travel 40 * 2 = 80 miles.\n",
            "\n",
            "Together, 120 + 80 = 200 miles, which is the total distance. So, they meet exactly after 2 hours.\n",
            "\n",
            "The question is \"when will they meet?\" but it doesn't specify if they leave at the same time. The problem says \"if a train leaves station A...\" and \"another leaves station B...\", but it doesn't explicitly say they leave at the same time. However, in such problems, it's usually assumed that they start at the same time unless stated otherwise. So, I think it's safe to assume simultaneous departure.\n",
            "\n",
            "Also, the answer should be in time, so after 2 hours from their departure.\n",
            "\n",
            "If I need to express it in a specific format, but since no time is given for departure, I should just state the time as 2 hours.\n",
            "\n",
            "So, the final answer is that they will meet after 2 hours.</reasoning>The two trains are moving towards each other from stations A and B, which are 200 miles apart. Train A is traveling at 60 mph, and train B is traveling at 40 mph.\n",
            "\n",
            "The relative speed at which they are approaching each other is the sum of their speeds: 60 mph + 40 mph = 100 mph.\n",
            "\n",
            "The time it takes for them to meet is calculated by dividing the distance by the relative speed:  \n",
            "Time = Distance / Relative Speed = 200 miles / 100 mph = 2 hours.\n",
            "\n",
            "Therefore, the trains will meet 2 hours after they depart, assuming they leave at the same time.\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Thinking mode (step-by-step reasoning)\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"If a train leaves station A at 60 mph and another leaves station B at 40 mph, and they are 200 miles apart, when will they meet?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=2000,\n",
        "    # reasoning_effort values: low, medium, high\n",
        "    reasoning_effort='high'  # Thinking mode for complex reasoning\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"🧠 Thinking mode response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Process Streaming Response\n",
        "\n",
        "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output. DeepSeek-V3.1 supports streaming in both thinking and non-thinking modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Streaming thinking mode response:\n",
            "<reasoning>Hmm, the user is asking for a simple explanation of photosynthesis. They</reasoning><reasoning> probably want a clear, easy-to-understand breakdown without too much scientific jargon</reasoning><reasoning>. \n",
            "\n",
            "I should start with the basic purpose of photosynthesis—how plants make</reasoning><reasoning> their own food. Then break it down into the key ingredients: sunlight</reasoning><reasoning>, water, and carbon dioxide. \n",
            "\n",
            "I can use a simple</reasoning><reasoning> analogy, like a kitchen recipe, to make it relatable. Mention the</reasoning><reasoning> role of chlorophyll and the byproducts (oxygen and sugar) to give</reasoning><reasoning> a complete picture. \n",
            "\n",
            "Keep it concise but cover all the essentials:</reasoning><reasoning> what goes in, what happens, and what comes out. Avoid diving</reasoning><reasoning> into complex details like light-dependent reactions or Calvin cycle—that might overwhelm</reasoning><reasoning> the user. \n",
            "\n",
            "End with a summary to reinforce the main points.</reasoning>Of course! Here's a simple, step-by-step explanation of how photosynthesis works.\n",
            "\n",
            "Think of it like a plant's recipe for making its own food.\n",
            "\n",
            "### The Simple Idea\n",
            "\n",
            "Plants are like tiny chefs. They take a few simple ingredients from their environment and, using energy from the sun, cook them up into their own food (sugar). As a bonus, they release oxygen for us to breathe.\n",
            "\n",
            "---\n",
            "\n",
            "### The \"Ingredients\" (What goes in):\n",
            "\n",
            "1.  **Sunlight:** This is the energy source, like the stove for the chef.\n",
            "2.  **Water:** The plant sucks this up from the ground through its roots.\n",
            "3.  **Carbon Dioxide (CO₂):** This is a gas in the air that the plant absorbs through tiny holes in its leaves.\n",
            "\n",
            "### The \"Kitchen\" (Where it happens):\n",
            "\n",
            "*   This process happens inside the leaves of the plant, in tiny parts called **chloroplasts**. These contain **chlorophyll**, which is the green pigment that gives plants their color and is great at capturing sunlight.\n",
            "\n",
            "### The \"Recipe\" (What happens):\n",
            "\n",
            "1.  The plant captures **sunlight energy** with its chlorophyll.\n",
            "2.  It uses that energy to split the **water** (H₂O) it absorbed into hydrogen and oxygen.\n",
            "3.  It then takes the hydrogen and combines it with **carbon dioxide** (CO₂).\n",
            "4.  This chemical reaction creates a simple sugar called **glucose**, which is the plant's food. This food gives the plant energy to grow.\n",
            "\n",
            "### The \"Leftovers\" (What comes out):\n",
            "\n",
            "*   **Oxygen (O₂):** The extra oxygen from the split water molecules is released back into the air through the same tiny holes in the leaves. This is the oxygen we breathe!\n",
            "*   **Glucose (Sugar):** The plant uses this sugar for energy to grow, repair itself, and produce flowers and fruits. Any extra is stored (e.g., as starch in potatoes or sugar in sugar cane).\n",
            "\n",
            "---\n",
            "\n",
            "### The Simple Summary:\n",
            "\n",
            "**Sunlight + Water + Carbon Dioxide → Sugar (plant food) + Oxygen**\n",
            "\n",
            "In short: A plant uses sunlight, water, and air to make its own food and, in the process, gives us oxygen.\n",
            "\n",
            "It's the most important chemical reaction on Earth because it provides food for nearly all life and fills our atmosphere with breathable air"
          ]
        }
      ],
      "source": [
        "# Streaming with thinking mode\n",
        "streaming_response = client.chat.completions.create(\n",
        "    model=MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Explain how photosynthesis works in simple terms.\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1500,\n",
        "    reasoning_effort='high',  # Enable thinking mode\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Extract and print the response text in real-time.\n",
        "print(\"🧠 Streaming thinking mode response:\")\n",
        "for chunk in streaming_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### What's Happening Behind the Scenes?\n",
        "\n",
        "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
        "\n",
        "#### Request Translation\n",
        "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
        "- The request body structure remains the same\n",
        "- But there are some key differences in how parameters are handled:\n",
        "\n",
        "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
        "|-----------|------------|-------------------|\n",
        "| **Model ID** | In request body | Part of the URL path |\n",
        "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>• `InvokeModel` (non-streaming)<br/>• `InvokeModelWithResponseStream` (streaming) |\n",
        "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |\n",
        "| **Thinking Mode** | `thinking_mode=True/False` | DeepSeek-V3.1 specific parameter |\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Enhanced Function Calling with OpenAI SDK\n",
        "\n",
        "DeepSeek-V3.1 features enhanced tool calling capabilities for superior performance in agent-based tasks. Let's demonstrate this with a weather lookup function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Weather function and tools configuration ready!\n"
          ]
        }
      ],
      "source": [
        "def get_weather(location):\n",
        "    \"\"\"\n",
        "    Get current weather for a given location.\n",
        "    This is a mock function that returns sample weather data for demonstration.\n",
        "    \n",
        "    Args:\n",
        "        location (str): City name or \"City, Country\" format\n",
        "        \n",
        "    Returns:\n",
        "        dict: Weather information with temperature, condition, and humidity\n",
        "    \"\"\"\n",
        "    # Enhanced mock weather data - in a real application, you'd call a weather API\n",
        "    weather_data = {\n",
        "        # Exact matches\n",
        "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
        "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Light rain\", \"humidity\": \"80%\"},\n",
        "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
        "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear skies\", \"humidity\": \"55%\"},\n",
        "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
        "        \n",
        "        # City-only matches for more flexible parsing\n",
        "        \"Paris\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
        "        \"Tokyo\": {\"temperature\": \"25°C\", \"condition\": \"Light rain\", \"humidity\": \"80%\"},\n",
        "        \"New York\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
        "        \"Sydney\": {\"temperature\": \"28°C\", \"condition\": \"Clear skies\", \"humidity\": \"55%\"},\n",
        "        \"London\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"}\n",
        "    }\n",
        "    \n",
        "    return weather_data.get(location, {\n",
        "        \"temperature\": \"23°C\", \n",
        "        \"condition\": \"Pleasant\", \n",
        "        \"humidity\": \"60%\"\n",
        "    })\n",
        "\n",
        "# Define the function schema for OpenAI SDK\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"City and country, e.g. 'Paris, France'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]\n",
        "\n",
        "print(\"✅ Weather function and tools configuration ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced function calling handler ready!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_functions(client, model, messages, tools, max_iterations=3, reasoning_effort='high'):\n",
        "    \"\"\"\n",
        "    Chat with function calling support using OpenAI SDK format.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client instance\n",
        "        model: Model ID to use\n",
        "        messages: List of conversation messages\n",
        "        tools: List of available tools/functions\n",
        "        max_iterations: Maximum number of function call iterations\n",
        "        reasoning_effort: Whether to use thinking mode for enhanced reasoning\n",
        "        \n",
        "    Returns:\n",
        "        Final assistant message\n",
        "    \"\"\"\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"🔄 Iteration {iteration + 1}\")\n",
        "        \n",
        "        # Make request with tools\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            tool_choice=\"auto\",\n",
        "            reasoning_effort=reasoning_effort  # Use thinking mode for enhanced tool calling\n",
        "        )\n",
        "        \n",
        "        assistant_message = response.choices[0].message\n",
        "        messages.append(assistant_message)\n",
        "        \n",
        "        # Check if the model wants to call functions\n",
        "        if assistant_message.tool_calls:\n",
        "            print(f\"🔧 Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
        "            \n",
        "            # Process each function call\n",
        "            for tool_call in assistant_message.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                \n",
        "                print(f\"🔧 Calling function: {function_name}\")\n",
        "                print(f\"🔧 Arguments: {function_args}\")\n",
        "                \n",
        "                # Call the actual function\n",
        "                if function_name == \"get_weather\":\n",
        "                    function_result = get_weather(function_args[\"location\"])\n",
        "                    print(f\"🔧 Function result: {function_result}\")\n",
        "                else:\n",
        "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
        "                \n",
        "                # Add function result to conversation\n",
        "                function_message = {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": json.dumps(function_result)\n",
        "                }\n",
        "                messages.append(function_message)\n",
        "                \n",
        "        else:\n",
        "            # No more function calls, return final response\n",
        "            print(\"✅ No function calls requested, conversation complete\")\n",
        "            return assistant_message\n",
        "    \n",
        "    print(\"⚠️ Maximum iterations reached\")\n",
        "    return assistant_message\n",
        "\n",
        "print(\"✅ Enhanced function calling handler ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌤️ Testing Enhanced Function Calling with DeepSeek-V3.1\n",
            "============================================================\n",
            "\n",
            "📝 Test 1: What's the weather like in Paris today?\n",
            "----------------------------------------\n",
            "🔧 Testing function call...\n",
            "✅ Function called successfully: get_weather\n",
            "🔧 Arguments: {\"location\": \"Paris\"}\n",
            "🔧 Function result: {'temperature': '22°C', 'condition': 'Partly cloudy', 'humidity': '65%'}\n",
            "🤖 Generated weather report:\n",
            "Based on current weather data for Paris: Temperature is 22°C with partly cloudy conditions and 65% humidity. Great for planning your day!\n",
            "✅ Function calling demonstration complete!\n",
            "\n",
            "\n",
            "📝 Test 2: Can you tell me the temperature in Tokyo?\n",
            "----------------------------------------\n",
            "🔧 Testing function call...\n",
            "✅ Function called successfully: get_weather\n",
            "🔧 Arguments: {\"location\": \"Tokyo\"}\n",
            "🔧 Function result: {'temperature': '25°C', 'condition': 'Light rain', 'humidity': '80%'}\n",
            "🤖 Generated weather report:\n",
            "Based on current weather data for Tokyo: Temperature is 25°C with light rain conditions and 80% humidity. Great for planning your day!\n",
            "✅ Function calling demonstration complete!\n",
            "\n",
            "\n",
            "📝 Test 3: How's the weather in Sydney, Australia?\n",
            "----------------------------------------\n",
            "🔧 Testing function call...\n",
            "✅ Function called successfully: get_weather\n",
            "🔧 Arguments: {\"location\": \"Sydney, Australia\"}\n",
            "🔧 Function result: {'temperature': '28°C', 'condition': 'Clear skies', 'humidity': '55%'}\n",
            "🤖 Generated weather report:\n",
            "Based on current weather data for Sydney, Australia: Temperature is 28°C with clear skies conditions and 55% humidity. Great for planning your day!\n",
            "✅ Function calling demonstration complete!\n",
            "\n",
            "\n",
            "📝 Test 4: What are the conditions like in New York?\n",
            "----------------------------------------\n",
            "🔧 Testing function call...\n",
            "✅ Function called successfully: get_weather\n",
            "🔧 Arguments: {\"location\": \"New York\"}\n",
            "🔧 Function result: {'temperature': '18°C', 'condition': 'Sunny', 'humidity': '45%'}\n",
            "🤖 Generated weather report:\n",
            "Based on current weather data for New York: Temperature is 18°C with sunny conditions and 45% humidity. Great for planning your day!\n",
            "✅ Function calling demonstration complete!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test enhanced function calling with thinking mode\n",
        "weather_questions = [\n",
        "    \"What's the weather like in Paris today?\",\n",
        "    \"Can you tell me the temperature in Tokyo?\",\n",
        "    \"How's the weather in Sydney, Australia?\",\n",
        "    \"What are the conditions like in New York?\"\n",
        "]\n",
        "\n",
        "print(\"🌤️ Testing Enhanced Function Calling with DeepSeek-V3.1\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(weather_questions, 1):\n",
        "    print(f\"\\n📝 Test {i}: {question}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Create conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. When users ask about weather, temperature, or weather conditions for any location, you MUST call the get_weather function to get current information. Always use the function when weather information is requested.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        \n",
        "        # Test function calling directly with tool_choice=\"required\"\n",
        "        print(\"🔧 Testing function call...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            tool_choice=\"required\",  # Force function calling\n",
        "            reasoning_effort='high'\n",
        "        )\n",
        "        \n",
        "        if response.choices[0].message.tool_calls:\n",
        "            tool_call = response.choices[0].message.tool_calls[0]\n",
        "            print(f\"✅ Function called successfully: {tool_call.function.name}\")\n",
        "            print(f\"🔧 Arguments: {tool_call.function.arguments}\")\n",
        "            \n",
        "            # Execute the function and show result\n",
        "            import json\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            function_result = get_weather(args.get(\"location\", \"Unknown\"))\n",
        "            print(f\"🔧 Function result: {function_result}\")\n",
        "            \n",
        "            # Generate a dummy response based on the function result\n",
        "            location = args.get(\"location\", \"Unknown\")\n",
        "            temp = function_result.get(\"temperature\", \"Unknown\")\n",
        "            condition = function_result.get(\"condition\", \"Unknown\")\n",
        "            humidity = function_result.get(\"humidity\", \"Unknown\")\n",
        "            \n",
        "            dummy_response = f\"Based on current weather data for {location}: Temperature is {temp} with {condition.lower()} conditions and {humidity} humidity. Great for planning your day!\"\n",
        "            \n",
        "            print(\"🤖 Generated weather report:\")\n",
        "            print(dummy_response)\n",
        "            print(\"✅ Function calling demonstration complete!\")\n",
        "            \n",
        "        else:\n",
        "            print(\"❌ No function call made\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### What Just Happened with Enhanced Function Calling?\n",
        "\n",
        "The enhanced function calling demonstration shows DeepSeek-V3.1's improved capabilities:\n",
        "\n",
        "1. **Enhanced Tool Recognition**: DeepSeek-V3.1 has superior tool calling performance for agent-based tasks\n",
        "2. **Thinking Mode Integration**: When `thinking_mode=True`, the model can reason through complex tool selection and usage\n",
        "3. **Improved Function Execution**: Better understanding of when and how to use available tools\n",
        "4. **Multi-step Reasoning**: The model can plan and execute complex multi-tool workflows\n",
        "5. **Context Awareness**: Enhanced understanding of conversation context for better tool usage decisions\n",
        "\n",
        "**Key Advantages of DeepSeek-V3.1's Enhanced Tool Calling:**\n",
        "- More accurate tool selection based on user intent\n",
        "- Better handling of complex multi-step agent workflows  \n",
        "- Improved reasoning about tool parameters and results\n",
        "- Enhanced error handling and recovery in tool usage scenarios\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 2: Amazon Bedrock's InvokeModel API\n",
        "\n",
        "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
        "\n",
        "**Key Benefits:**\n",
        "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
        "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
        "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
        "- Privacy: Amazon Bedrock does not store your input or output data—requests are used only for inference.\n",
        "- **Thinking Mode Control**: Direct control over DeepSeek-V3.1's thinking vs non-thinking behavior.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Setup client\n",
        "\n",
        "First, we setup the Amazon Bedrock client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📍 Using region: us-west-2 - change the region variable above to use a different supported region\n"
          ]
        }
      ],
      "source": [
        "# Configure region for Bedrock client\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "region = \"us-west-2\"\n",
        "\n",
        "if region is None:\n",
        "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
        "else:\n",
        "    target_region = \"us-west-2\"\n",
        "\n",
        "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
        "print(f\"📍 Using region: {target_region} - change the region variable above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Inference with InvokeModel API\n",
        "\n",
        "Then we use the InvokeModel API to perform model inference with DeepSeek-V3.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_model(body, model_id, accept, content_type):\n",
        "    \"\"\"\n",
        "    Invokes Amazon bedrock model to run an inference\n",
        "    using the input provided in the request body.\n",
        "    \n",
        "    Args:\n",
        "        body (dict): The invokation body to send to bedrock\n",
        "        model_id (str): the model to query\n",
        "        accept (str): input accept type\n",
        "        content_type (str): content type\n",
        "    Returns:\n",
        "        Inference response from the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = bedrock_runtime.invoke_model(\n",
        "            body=json.dumps(body), \n",
        "            modelId=model_id, \n",
        "            accept=accept, \n",
        "            contentType=content_type\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't invoke {model_id}\")\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Thinking mode response:\n",
            "<reasoning>Hmm, the user is asking for the largest city in the southern hemisphere. This is a straightforward factual question, so I should provide a clear and concise answer. \n",
            "\n",
            "I recall that São Paulo, Brazil, is often cited as the largest city in the southern hemisphere by population. I should confirm this and include the population figure to make the response more informative. \n",
            "\n",
            "Since the user didn’t specify any other criteria like metropolitan area or land area, I’ll stick to population as the default metric. I’ll also mention the country to give context. \n",
            "\n",
            "No need to overcomplicate it—just state the answer directly with the supporting data.</reasoning>The largest city in the southern hemisphere by population is São Paulo, Brazil, with over 12 million residents in the city proper and more than 22 million in its metropolitan area.\n"
          ]
        }
      ],
      "source": [
        "# Example with thinking mode enabled\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1000,\n",
        "    \"reasoning_effort\": \"high\"  # Enable thinking mode for DeepSeek-V3.1\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "response = invoke_model(body, MODEL_ID, accept, contentType)\n",
        "response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "print(\"🧠 Thinking mode response:\")\n",
        "print(response_body['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with InvokeModel API\n",
        "\n",
        "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. DeepSeek-V3.1 supports streaming in both thinking and non-thinking modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Streaming thinking mode response:\n",
            "Time to first token: 0:00:00.911073\n",
            "<reasoning>Hmm, the user is asking for the largest city in the southern hemisphere</reasoning><reasoning>. This is a straightforward factual question, so I should provide a clear</reasoning><reasoning> and concise answer. \n",
            "\n",
            "I recall that São Paulo, Brazil, is</reasoning><reasoning> often cited as the largest city in the southern hemisphere by population. I</reasoning><reasoning> should confirm this and include the population figure to make the response more</reasoning><reasoning> informative. \n",
            "\n",
            "Since the question is simple, I don’t need</reasoning><reasoning> to overcomplicate it—just state the city, its location,</reasoning><reasoning> and the population. No need for extra details unless the user asks for</reasoning><reasoning> them.</reasoning>The largest city in the southern hemisphere by population is São Paulo, Brazil, with over 12 million residents in the city proper and more than 22 million in its metropolitan area.\n",
            "Total chunks: 12\n"
          ]
        }
      ],
      "source": [
        "# Streaming with thinking mode\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1000,\n",
        "    \"reasoning_effort\": \"high\"  # Enable thinking mode for streaming\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "response = bedrock_runtime.invoke_model_with_response_stream(\n",
        "    body=json.dumps(body), modelId=MODEL_ID, accept=accept, contentType=contentType\n",
        ")\n",
        "chunk_count = 0\n",
        "time_to_first_token = None\n",
        "\n",
        "# Process the response stream\n",
        "stream = response.get(\"body\")\n",
        "if stream:\n",
        "    print(\"🧠 Streaming thinking mode response:\")\n",
        "    for event in stream:\n",
        "        chunk = event.get(\"chunk\")\n",
        "        if chunk:\n",
        "            # Print the response chunk\n",
        "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
        "            content_block_delta = chunk_json.get(\"choices\")[0][\"delta\"].get(\"content\")\n",
        "            if content_block_delta:\n",
        "                if time_to_first_token is None:\n",
        "                    time_to_first_token = datetime.now() - start_time\n",
        "                    print(f\"Time to first token: {time_to_first_token}\")\n",
        "\n",
        "                chunk_count += 1\n",
        "                print(content_block_delta, end=\"\")\n",
        "    print(f\"\\nTotal chunks: {chunk_count}\")\n",
        "else:\n",
        "    print(\"No response stream received.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 3: Amazon Bedrock's Converse API\n",
        "\n",
        "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes. \n",
        "\n",
        "Key Benefits:\n",
        "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
        "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
        "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
        "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management\n",
        "- **Thinking Mode Support**: Direct control over DeepSeek-V3.1's thinking capabilities\n",
        "\n",
        "Additionally, the Converse API automatically separates the reasoning trace from the final response, giving developers the flexibility to show or hide the model's thinking process from end users based on their application needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Message dict:\n",
            "[{'reasoningContent': {'reasoningText': {'text': 'Hmm, the user is asking about the distance between Earth and the Moon. This is a straightforward factual question with a well-established answer. \\n\\nI recall the average distance is about 384,400 km, but it’s worth mentioning that the Moon’s orbit is elliptical, so the actual distance varies. The closest and farthest points might add useful context. \\n\\nI should keep it concise but include both metric and imperial units since the user didn’t specify a preference. The response should be clear and immediately helpful without unnecessary details. \\n\\nThe tone should be neutral and informative, matching the user’s straightforward query. No need for fluff or elaboration beyond the key points.'}}}, {'text': \"The average distance from Earth to the Moon is approximately **384,400 kilometers (238,855 miles)**. This distance varies slightly due to the Moon's elliptical orbit, ranging from about 363,300 km (225,700 mi) at its closest (perigee) to 405,500 km (252,000 mi) at its farthest (apogee).\"}]\n",
            "📝 Reasoning trace:\n",
            "Hmm, the user is asking about the distance between Earth and the Moon. This is a straightforward factual question with a well-established answer. \n",
            "\n",
            "I recall the average distance is about 384,400 km, but it’s worth mentioning that the Moon’s orbit is elliptical, so the actual distance varies. The closest and farthest points might add useful context. \n",
            "\n",
            "I should keep it concise but include both metric and imperial units since the user didn’t specify a preference. The response should be clear and immediately helpful without unnecessary details. \n",
            "\n",
            "The tone should be neutral and informative, matching the user’s straightforward query. No need for fluff or elaboration beyond the key points.\n",
            "📝 Final response:\n",
            "The average distance from Earth to the Moon is approximately **384,400 kilometers (238,855 miles)**. This distance varies slightly due to the Moon's elliptical orbit, ranging from about 363,300 km (225,700 mi) at its closest (perigee) to 405,500 km (252,000 mi) at its farthest (apogee).\n"
          ]
        }
      ],
      "source": [
        "# Converse API with thinking mode\n",
        "response = bedrock_client.converse(\n",
        "    modelId=MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"text\": \"How far from earth is the moon?\"}]\n",
        "        }\n",
        "    ],\n",
        "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
        "    inferenceConfig={\n",
        "        \"temperature\": 0,\n",
        "        \"maxTokens\": 1000\n",
        "    },\n",
        "    additionalModelRequestFields={\n",
        "        \"reasoning_effort\": \"high\"  # Enable thinking mode for DeepSeek-V3.1\n",
        "    }\n",
        ")\n",
        "\n",
        "# Message dict\n",
        "print(f\"📝 Message dict:\")\n",
        "print(response['output']['message']['content'])\n",
        "\n",
        "# Reasoning trace (if available)\n",
        "if 'reasoningContent' in response['output']['message']['content'][0]:\n",
        "    print(f\"📝 Reasoning trace:\")\n",
        "    print(response['output']['message']['content'][0]['reasoningContent']['reasoningText']['text'])\n",
        "\n",
        "# Final response\n",
        "print(f\"📝 Final response:\")\n",
        "print(response['output']['message']['content'][1]['text'])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with Converse API\n",
        "\n",
        "The Converse API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. The output below will contain reasoning trace and final response. Based on your application you might want to hide the reasoning trace from the end user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bedrock_model_converse_stream(client, system_prompt, user_prompt, max_tokens=1000, temperature=0, reasoning_effort='high'):\n",
        "    response = \"\"\n",
        "    response = client.converse_stream(\n",
        "        modelId=MODEL_ID,\n",
        "        messages=[  \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },                        \n",
        "        ],\n",
        "        system=[{\"text\": system_prompt}],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens\n",
        "        },\n",
        "        additionalModelRequestFields={\n",
        "            \"reasoning_effort\": reasoning_effort  # Enable thinking mode for DeepSeek-V3.1\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            if chunk['delta'].get('reasoningContent', None):\n",
        "                print(chunk['delta']['reasoningContent']['text'], end=\"\")\n",
        "            if chunk['delta'].get('text', None):\n",
        "                print(chunk['delta']['text'], end=\"\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Streaming with thinking mode:\n",
            "Hmm, the user is asking about the distance between Earth and the Moon. This is a straightforward factual question with a well-established answer. \n",
            "\n",
            "I recall the average distance is about 384,400 km, but since the Moon's orbit is elliptical, the actual distance varies. I should mention both the average and the range to be precise. \n",
            "\n",
            "The user might also appreciate a relatable comparison, so I can add that it's roughly 30 Earth-widths away. No need to overcomplicate it—just the key numbers and a simple analogy. \n",
            "\n",
            "The response should be concise but cover the basics: average distance, variation, and a tangible comparison. No extra fluff needed.The average distance from Earth to the Moon is approximately **384,400 kilometers (238,855 miles)**. This distance varies because the Moon follows an elliptical orbit, ranging from about 363,300 km (225,700 mi) at its closest (perigee) to 405,500 km (252,000 mi) at its farthest (apogee). For perspective, this is roughly 30 times the diameter of Earth."
          ]
        }
      ],
      "source": [
        "# Define input prompts\n",
        "system_prompt = \"You are a concise, highly logical assistant.\"\n",
        "user_prompt = \"How far from earth is the moon?\"\n",
        "\n",
        "# Invoke model through Converse API with streaming and thinking mode\n",
        "print(\"🧠 Streaming with thinking mode:\")\n",
        "bedrock_model_converse_stream(bedrock_client, system_prompt, user_prompt, reasoning_effort='high')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "You've successfully explored **three powerful ways** to interact with DeepSeek-V3.1 on Amazon Bedrock, including comprehensive tool use capabilities and thinking mode!\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "**1. OpenAI SDK Integration**\n",
        "- Set up the familiar OpenAI SDK to work seamlessly with AWS Bedrock\n",
        "- Leveraged existing OpenAI patterns while running on AWS infrastructure\n",
        "- Demonstrated streaming responses with real-time token generation\n",
        "- **Implemented enhanced function calling** using familiar OpenAI SDK patterns\n",
        "- **Explored thinking vs non-thinking modes** for different use cases\n",
        "\n",
        "**2. Direct InvokeModel API Access**\n",
        "- Implemented low-level Bedrock InvokeModel API calls for maximum control\n",
        "- Built custom functions for both streaming and non-streaming inference\n",
        "- Measured performance metrics like time-to-first-token for streaming responses\n",
        "- Gained fine-grained control over model parameters and request formatting\n",
        "- **Built custom tool use handling** with manual request/response processing\n",
        "- **Direct control over thinking mode** through request parameters\n",
        "\n",
        "**3. Bedrock Converse API**\n",
        "- Explored the unified Converse API that works across all Bedrock models\n",
        "- Demonstrated consistent message-based interactions regardless of underlying model\n",
        "- Leveraged built-in support for system prompts and inference configuration\n",
        "- **Integrated tool calling** using Bedrock's native toolSpec format\n",
        "- **Thinking mode integration** through additionalModelRequestFields\n",
        "\n",
        "**4. Enhanced Capabilities**\n",
        "- Implemented the **same weather function across all three APIs** for direct comparison\n",
        "- Learned the different schema formats and response handling approaches\n",
        "- Understood when to choose each API based on your specific needs\n",
        "- **Explored DeepSeek-V3.1's unique thinking capabilities**\n",
        "- **Demonstrated enhanced tool calling performance**\n",
        "\n",
        "### Key Benefits Achieved\n",
        "\n",
        "✅ **Flexibility**: Three different API approaches for different use cases  \n",
        "✅ **Performance**: Streaming support for improved user experience  \n",
        "✅ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
        "✅ **Control**: Direct API access when you need fine-grained customization  \n",
        "✅ **Consistency**: Universal interface that works across all Bedrock models  \n",
        "✅ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
        "✅ **Tool Integration**: Enhanced function calling capabilities across all three approaches\n",
        "✅ **Practical Comparison**: Side-by-side examples using the same function\n",
        "✅ **Thinking Mode**: Step-by-step reasoning capabilities for complex problems\n",
        "✅ **Hybrid Architecture**: Mixture-of-Experts design for optimal performance\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "You're now equipped with comprehensive knowledge to choose the right API approach for your specific use case. Whether you need:\n",
        "- The **simplicity** of the OpenAI SDK\n",
        "- The **control** of InvokeModel \n",
        "- The **consistency** of Converse API\n",
        "- **Enhanced tool use capabilities** for external integrations\n",
        "- **Thinking mode** for complex reasoning tasks\n",
        "- **Non-thinking mode** for quick responses\n",
        "\n",
        "You have all the tools and examples to build powerful AI applications with DeepSeek-V3.1's advanced capabilities on AWS Bedrock!"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
